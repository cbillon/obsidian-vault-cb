---
link: https://enix.io/fr/blog/cherie-j-ai-retreci-docker-part3/
byline: Enix
excerpt: Cette sÃ©rie de trois articles prÃ©sente des solutions pour optimiser la taille des images Docker. On parle dans ce dernier article de standardisation d'image de base, d'optimisation d'assets, de systÃ¨mes de build alternatifs comme DockerSlim ou Bazel ou encore de la distribution NixOS.
slurped: 2024-05-13T13:41:04.707Z
title: ChÃ©rie, j'ai rÃ©trÃ©ci Docker - part 3/3
tags:
  - docker
  - image
  - size
---

La voilÃ  ! La **derniÃ¨re partie** de notre sÃ©rie sur lâ€™**optimisation de la taille des images Docker** !

Si vous avez ratÃ© les deux premiers Ã©pisodes, vous pouvez les retrouver ici :

- [â€œChÃ©rie, jâ€™ai rÃ©trÃ©ci Docker !â€, Part 1/3](https://enix.io/fr/blog/cherie-j-ai-retreci-docker-part1/)
    
- [â€œChÃ©rie, jâ€™ai rÃ©trÃ©ci Docker !â€, Part 2/3](https://enix.io/fr/blog/cherie-j-ai-retreci-docker-part2/)
    

## Introduction

Dans les **deux premiÃ¨res parties**, on avait couvert les mÃ©thodes les plus courantes pour optimiser la taille de nos images Docker. On avait vu que les amÃ©liorations les plus convaincantes sont gÃ©nÃ©ralement obtenues grÃ¢ce aux builds multi-stage avec des images basÃ©es sur Alpine, et quâ€™on pouvait faire encore mieux (au dÃ©triment de la maintenabilitÃ©) avec des builds statiques.

Dans cette **derniÃ¨re partie**, on va voir comment on peut aller encore plus loin. On va parler de **standardisation dâ€™images de base**, de **â€œstrippingâ€ de binaires**, dâ€™optimisation dâ€™assets, de **systÃ¨mes de build alternatifs** comme _DockerSlim_, _Bazel_ ou encore de la distribution _NixOS_.

On parlera aussi de petits dÃ©tails mis de cÃ´tÃ© jusquâ€™Ã  prÃ©sent mais qui peuvent Ãªtre importants, comme les fichiers de **timezone** ou les **certificats**.

## Partir sur de bonnes bases

VoilÃ  une mÃ©thode qui va permettre de faire des Ã©conomies significatives dÃ¨s que vos nÅ“uds font tourner plusieurs conteneurs en parallÃ¨le : le **partage de couches**.

Les images Docker sont constituÃ©es de _layers_ (des couches, en bon franÃ§ais). Et chaque couche peut ajouter, supprimer ou modifier des fichiers, exactement comme les _commits_ dans un dÃ©pÃ´t de code ; ou encore, comme une classe qui hÃ©rite dâ€™une autre classe. Lorsquâ€™on exÃ©cute un `docker build`, chaque ligne du Dockerfile va gÃ©nÃ©rer une couche. Et **lorsquâ€™on transfÃ¨re une image, on transfÃ¨re seulement les couches qui nâ€™existent pas encore** sur la destination.

Si de nombreuses images partagent les mÃªmes couches, Docker nâ€™a besoin de transfÃ©rer et stocker la couche quâ€™une seule fois pour lâ€™ensemble des images. Cela permet dâ€™**Ã©conomiser de la bande passante rÃ©seau et de lâ€™espace de stockage**.

Suivant le _storage driver_ utilisÃ© par Docker, les couches peuvent Ã©galement permettre dâ€™Ã©conomiser des I/O disques et de la mÃ©moire. Lorsque de nombreux conteneurs ont besoin de lire les mÃªmes fichiers dâ€™une couche, le systÃ¨me va en effet lire et mettre en cache ces fichiers une seule fois. (Câ€™est le cas notamment avec les drivers _overlay2_ et _aufs_).

On voit assez vite comment on peut **optimiser les ressources rÃ©seau, le disque et la mÃ©moire sur les noeuds qui hÃ©bergent les conteneurs** : il faut que nos conteneurs (plus prÃ©cisÃ©ment, leurs images) partagent ces couches au maximum. LÃ  oÃ¹ les choses se compliquent, câ€™est que cette mÃ©thode peut aller totalement Ã  lâ€™encontre des recommandations que je vous ai donnÃ©es dans les deux premiers articles de cette sÃ©rie !

Prenons un exemple oÃ¹ on build de superbes images ultra optimisÃ©es avec des binaires statiques. Ces binaires peuvent prendre au final 10 fois plus dâ€™espace que leurs Ã©quivalents dynamiques. Je vous mets quelques scÃ©narios oÃ¹ on fait tourner 10 conteneurs, chacun utilisant une image diffÃ©rente avec lâ€™un de ces binaires :

ScÃ©nario 1: binaires statiques avec lâ€™image `scratch`

- Taille de chaque image : 10 Mo
- Taille des 10 images: 100 Mo (quel calcul !)

ScÃ©nario 2: binaires dynamiques avec lâ€™image `ubuntu` (64 Mo)

- Taille individuelle de chaque image : 65 Mo
- RÃ©partition de la taille de chaque image : 64 Mo pour `ubuntu` + 1 Mo pour le binaire spÃ©cifique
- Utilisation disque totale : 74 Mo (10 x 1 Mo pour les couches spÃ©cifiques + 64 Mo pour les couches partagÃ©es)

ScÃ©nario 3: binaires dynamiques avec lâ€™image `alpine` (5.5 Mo)

- Taille individuelle de chaque image : 6,5 Mo
- RÃ©partition de taille de chaque image : 5.5 Mo pour `alpine` + 1 Mo pour le binaire spÃ©cifique
- Utilisation disque totale : 15.5 Mo

A premiÃ¨re vue, les binaires statiques optimisÃ©s Ã©taient une superbe idÃ©e. Mais on voit bien ici que dans un contexte avec de nombreux conteneurs, ils sont en fait totalement contre productifs. Les images utilisent plus dâ€™espace disque, elles sont plus longues Ã  transfÃ©rer et elles utilisent plus de RAM. On peut difficilement faire pire par rapport Ã  notre objectif initial !

Un dÃ©tail important : pour que ces scÃ©narios de partage des couches fonctionnent, on doit sâ€™assurer que toutes les images utilisent exactement la mÃªme image de base. Si on a plusieurs images qui utilisent `centos` et dâ€™autres qui utilisent `debian`, on nâ€™optimise rien du tout. Ce nâ€™est pas mieux si on utilise `ubuntu:16.04` et `ubuntu:18.04`. Et ce nâ€™est pas mieux non plus si on utilise deux versions diffÃ©rentes de `ubuntu:18.04` !

Cela signifie Ã©galement quâ€™Ã  chaque fois quâ€™une image de base est mise Ã  jour, on doit rebuild toutes nos images pour conserver une cohÃ©rence entre nos conteneurs.

Ã‡a veut aussi dire quâ€™on va avoir besoin dâ€™une **bonne organisation et dâ€™une bonne communication entre les Ã©quipes**. Vous vous dites peut-Ãªtre que Ã§a nâ€™est pas un problÃ¨me technique, Ã§a. Câ€™est vrai, mais justement : câ€™est dâ€™autant plus difficile Ã  rÃ©soudre ! Il ne suffit plus de trouver la meilleure solution pour _son_ image. Si on veut avoir les meilleurs rÃ©sultats, il faut se mettre dâ€™accord avec ses collÃ¨gues pour adopter une image qui convienne Ã  tout le monde. Par exemple, peut-Ãªtre que vous ou votre Ã©quipe voulez absolument utiliser une image de base Debian, alors que dâ€™autres Ã©quipes prÃ©fÃ¨reraient utiliser une image de base Ubuntu. Il va donc falloir impliquer les autres et parler ensemble pour faire le meilleur choix possible. Dans ce genre de discussion, le plus important nâ€™est pas toujours dâ€™avoir les meilleurs arguments, mais dâ€™Ãªtre prÃªtÂ·e Ã  changer dâ€™avis â€¦ Car si chacunÂ·e campe sur ses positions, il nâ€™y aura pas de consensus. MoralitÃ© : si vous voulez vraiment optimiser la taille de vos images au maximum, les solutions les plus efficaces seront aussi **obtenues grÃ¢ce Ã  des compÃ©tences interpersonnelles et pas uniquement techniques**.

Enfin, il y a un cas spÃ©cifique oÃ¹ les images statiques continuent dâ€™avoir de lâ€™intÃ©rÃªt. Câ€™est lorsquâ€™on sait Ã  lâ€™avance que nos images vont Ãªtre exÃ©cutÃ©es dans des environnements hÃ©tÃ©rogÃ¨nes, ou lorsquâ€™elles seront la seule chose qui tourne sur un nÅ“ud donnÃ©. Dans ce cas, aucun partage de couche nâ€™est possible entre les conteneurs et donc aucune optimisation de ce type.

## Nettoyage de binaires et compression

Dans ce chapitre, on aborde des **techniques complÃ©mentaires** qui ne sont pas spÃ©cifiques aux conteneurs mais qui peuvent nous faire Ã©conomiser quelques mÃ©gaoctets (mais parfois seulement quelques kilooctets).

### Nettoyage de vos binaires

Par dÃ©faut, la plupart des compilateurs gÃ©nÃ¨rent des binaires avec des symboles de debug. Câ€™est pratique en cas dâ€™incident, mais Ã§a nâ€™est pas strictement nÃ©cessaires Ã  lâ€™exÃ©cution. Lâ€™outil `strip` supprime ces symboles. Je vous accorde que lâ€™Ã©conomie rÃ©alisÃ©e nâ€™est pas transcendante, mais Ã§a peut Ãªtre utile si vous Ãªtes dans une situation oÃ¹ chaque octet compte.

### Et pour les fichiers mÃ©dias dans nos conteneurs ?

Il y a quelques questions utiles Ã  se poser dans le cas oÃ¹ notre image de conteneur contient des fichiers mÃ©dia (images, animations, sons, etc). Peut-on rÃ©duire leur taille, par exemple en utilisant des formats de fichier ou des codecs diffÃ©rents ? Peut-on les hÃ©berger Ã  lâ€™extÃ©rieur de lâ€™image pour diminuer la taille de lâ€™image Ã  distribuer ? Cette derniÃ¨re option est particuliÃ¨rement intÃ©ressante si le code change souvent par rapport aux assets. Dans ce cas prÃ©cis, câ€™est malin dâ€™**Ã©viter de redistribuer les assets Ã  chaque nouvelle release de code**.

### Compression : la fausse bonne idÃ©e

Une solution qui nous vient toujours Ã  lâ€™esprit pour optimiser la taille de fichiers est la compression. Si on veut diminuer la taille de nos images, alors pourquoi ne pas compresser nos fichiers ? Des assets HTML, javascript ou autres CSS devraient bien se compresser avec zip ou gzip. Il existe mÃªme des mÃ©thodes plus efficaces comme bzip2, 7z ou encore Izma.

A premiÃ¨re vue, câ€™est un moyen simple pour rÃ©duire la taille des images. Et si on prÃ©voit de servir ces assets de maniÃ¨re compressÃ©e, alors pourquoi pas. Mais si on a besoin de dÃ©compresser ces assets pour les utiliser, on va au contraire gÃ¢cher des ressources. Voyons pourquoiâ€¦

**Les couches Docker sont dÃ©jÃ  compressÃ©es** avant dâ€™Ãªtre transfÃ©rÃ©es, on ne gagnerait donc rien en jouant sur la compression de nos images. Et si on compte dÃ©compresser les fichiers au lancement du conteneur, lâ€™utilisation du disque sera encore pire : nous allons stocker Ã  la fois les versions compressÃ©es et dÃ©compressÃ©es ! Pire encore, on ne peut pas optimiser la taille via le partage de couche quâ€™on a vu juste avant : ces fichiers nâ€™Ã©tant dÃ©compressÃ©s que lorsquâ€™on exÃ©cute les conteneurs, ils ne seront pas partagÃ©s.

Et quâ€™en est-il de [UPX](https://upx.github.io/) ? Si vous nâ€™Ãªtes pas familier avec UPX, câ€™est un trÃ¨s bon outil pour rÃ©duire la taille des binaires. Il compresse le binaire et y ajoute une petite surcouche afin de le dÃ©compresser de faÃ§on transparente lors du lancement. RÃ©sultat : des binaires plus petits, en Ã©change dâ€™une courte attente au dÃ©marrage (Ã  cause de la dÃ©compression).

Mais si on veut rÃ©duire la taille de nos conteneurs, UPX sera (lui aussi) contre productif.

Tout dâ€™abord, comme pour la compression classique, le disque et lâ€™usage rÃ©seau ne vont pas Ãªtre optimisÃ©s car UPX nâ€™apportera rien en complÃ©ment de la compression native des couches Docker.

Ensuite, lorsquâ€™on lance un binaire classique, seules les portions nÃ©cessaires sont chargÃ©es en mÃ©moire (comme lorsquâ€™on utilise `mmap()`). En revanche, lorsquâ€™on lance un binaire compressÃ© avec UPX, tout le binaire doit Ãªtre dÃ©compressÃ© en mÃ©moire. Ceci induit une utilisation mÃ©moire plus Ã©levÃ©e et des temps de dÃ©marrage rallongÃ©s, en particulier avec des runtimes comme Go qui tendent Ã  gÃ©nÃ©rer des binaires plus gros.

(Quand je prÃ©parais lâ€™examen pour devenir Certified Kubernetes Administrator, jâ€™ai montÃ© un lab de machines virtuelles KVM, dans lesquelles jâ€™utilisais le binaire [hyperkube](https://stackoverflow.com/questions/33953254/what-is-hyperkube/33967582#33967582). Comme câ€™est un trÃ¨s gros binaire, jâ€™ai voulu gagner de la place en le passant Ã  la moulinette avec UPX. Le rÃ©sultat Ã  Ã©tÃ© catastrophique : jâ€™ai rÃ©ussi Ã  diminuer la taille disque de mes VMs, mais lâ€™utilisation mÃ©moire a explosÃ©.)

## Quelques techniques exotiques !

Il y a pas mal dâ€™autres outils qui peuvent nous aider Ã  rÃ©duire la taille de nos images Docker. La liste ci-dessous prÃ©sente quelques uns de ces outils, mais elle nâ€™est pas exhaustive.

### DockerSlim

[DockerSlim](https://github.com/docker-slim/docker-slim) sâ€™appuie sur une technique de rÃ©duction de la taille des images qui est presque â€¦ magique. Je ne sais pas exactement comment Ã§a marche dans le dÃ©tail (au delÃ  de la [description du design](https://github.com/docker-slim/docker-slim#design) dans le README), du coup je vais faire des suppositions avec vous.

Jâ€™ai lâ€™impression que DockerSlim exÃ©cute notre conteneur, observe quels fichiers ont Ã©tÃ© consultÃ©s pendant son exÃ©cution, et supprime ceux qui nâ€™ont pas Ã©tÃ© accÃ©dÃ©s. En se basant sur cette hypothÃ¨se, je me suis dis quâ€™il fallait vraiment faire attention lorsquâ€™on utilise DockerSlim, car de nombreux runtimes et frameworks chargent des fichiers dynamiquement, ou Ã  la volÃ©e, seulement la premiÃ¨re fois quâ€™ils sont nÃ©cessaires.

Pour vÃ©rifier cette hypothÃ¨se, jâ€™ai essayÃ© DockerSlim pour une application Django trÃ¨s simple. DockerSlim a rÃ©duit la taille de 200 Mo Ã  30 Mo. Pas mal ! La _home_ de lâ€™application fonctionnait ; en revanche, de nombreux liens Ã©taient cassÃ©s. Je suppose que les _templates_ (qui sont typiquement chargÃ©s Ã  la demande par Django, sauf configuration spÃ©cifique) nâ€™ont pas tous Ã©tÃ© dÃ©tectÃ©s par DockerSlim et nâ€™ont donc pas Ã©tÃ© inclus dans lâ€™image finale. Pour la petite histoire, mÃªme les pages dâ€™erreurs Ã©taient cassÃ©es, probablement car les modules utilisÃ©s pour envoyer et afficher les exceptions avaient Ã©tÃ© zappÃ©s eux aussi. Je pense que nâ€™importe quel code Python avec des modules dynamiques rencontrerait le mÃªme problÃ¨me.

Est-ce que Ã§a veut dire que je dÃ©conseille DockerSlim ? Non ! Dans certains cas, il peut faire des merveilles. **Comme pour tous les outils puissants, il faut comprendre comment il fonctionne pour savoir quand et comment lâ€™utiliser** sans mauvaise surprise.

### Distroless

Les images [Distroless](https://github.com/GoogleContainerTools/distroless) sont une collection dâ€™images minimales construites sans les _package managers_ classiques de distributions Linux et avec des outils extÃ©rieurs. On a **des images trÃ¨s petites**, mais un peu spartiates Ã  mon goÃ»t : il manque les outils comme `ps`, `ls`, etc., qui sont bien pratiques en phase de debug.

Pour ma part, je prÃ©fÃ¨re avoir un _package manager_ et une distribution familiÃ¨re. Qui sait si je ne vais pas avoir besoin dâ€™un outil supplÃ©mentaire pour investiguer en live un problÃ¨me de conteneur ? Alpine fait seulement 5,5 Mo et me permet dâ€™installer tout ce dont je peux avoir besoin. Pour 5,5 Mo, je ne pense pas que Ã§a vaille la peine de se priver de cette flexibilitÃ©.

De votre cÃ´tÃ©, si vous maÃ®trisez des mÃ©thodes permettant dâ€™analyser vos conteneurs sans jamais avoir besoin dâ€™exÃ©cuter des programmes au sein du conteneur, vous pouvez clairement optimiser vos images en utilisant des bases Distroless.

Au passage, les images basÃ©es sur Alpine sont souvent plus petites que leurs Ã©quivalentes Distroless. On peut alors se demander quel est lâ€™intÃ©rÃªt de Distroless â€¦ Il y en a au moins deux.

Tout dâ€™abord, la **sÃ©curitÃ©**. Les images Distroless sont minimales. Moins de choses dans lâ€™image signifie de facto moins de vulnÃ©rabilitÃ©s potentielles.

Ensuite, les images Distroless sont construites avec **Bazel**. Si vous voulez apprendre et expÃ©rimenter avec Bazel, ces images sont donc de solides exemples pour dÃ©marrer. Vous vous demandez peut-Ãªtre ce quâ€™est Bazel exactement ? Et bien, jâ€™ai justement prÃ©vu dâ€™en parler dans la section dâ€™aprÃ¨s !

### Bazel (et autres builders alternatifs)

Il existe des systÃ¨mes de build qui nâ€™utilisent mÃªme pas de Dockerfiles. [Bazel](https://bazel.build/) est lâ€™un dâ€™entre eux. La force de Bazel est de **permettre de dÃ©finir des dÃ©pendances complexes entre notre code source et les binaires quâ€™il crÃ©e**, un peu comme un Makefile. Ã‡a nous permet de re-build uniquement ce qui est nÃ©cessaire, que ce soit dans notre code (suite Ã  une petite modification locale) ou dans nos images de base (de sorte quâ€™on puisse appliquer un patch ou mettre Ã  jour une bibliothÃ¨que sans devoir rebuild toutes nos images). Bazel permet aussi de rÃ©aliser des tests unitaires avec la mÃªme efficacitÃ©, et de nâ€™exÃ©cuter que les tests des modules affectÃ©s par nos changements de code.

**Bazel est particuliÃ¨rement efficace pour les programmes trÃ¨s gros et trÃ¨s complexes**. Quand on atteint une certaine complexitÃ©, le build et les tests peuvent prendre des heures. Puis des jours. ArrivÃ©s Ã  ce stade, typiquement, on dÃ©ploie des fermes de build et de test pour rÃ©gler le problÃ¨me. On retrouve des temps plus raisonnables, mais Ã§a peut quand mÃªme prendre des heures, et surtout, Ã§a demande Ã©normÃ©ment de ressources, et Ã§a nâ€™est souvent plus possible de faire tourner tous les tests en local. Câ€™est Ã  ce moment lÃ  que Ã§a vaut le coup de dÃ©gainer un outil comme Bazel, parce quâ€™il est **capable de compiler et de tester uniquement le strict nÃ©cessaire**. Et au lieu de prendre des heures ou des jours, Ã§a ne prend que quelques minutes.

Parfait ! Ã‡a a lâ€™air mortel ce truc, allez, on migre tout sur Bazel !

Hop hop hop. Pas si vite.

Utiliser Bazel nÃ©cessite dâ€™apprendre un systÃ¨me de build complet et complexe. Beaucoup plus complexe que les Dockerfiles ; mÃªme en comptant les astuces de builds multi-stage dont on a parlÃ© dans les premiers articles avec les subtilitÃ©s sur les bibliothÃ¨ques statiques et dynamiques.

**Maintenir ce systÃ¨me de build et les recettes associÃ©es nâ€™est pas une mince affaire et nÃ©cessite un travail consÃ©quent**. Ã€ titre personnel, je nâ€™ai pas de retour dâ€™expÃ©rience Ã  faire sur Bazel, mais dâ€™aprÃ¨s ce que jâ€™ai vu autour de moi, la maintenance dâ€™un systÃ¨me de build et de test basÃ© sur Bazel peut facilement occuper une personne senior Ã  temps plein.

Pour les organisations qui comptent des centaines de dÃ©veloppeurs qui perdent un temps considÃ©rable Ã  cause de leurs build et de leurs tests, câ€™est probablement une bonne idÃ©e dâ€™investir dans Bazel. A lâ€™inverse, pour une petite start-up ou une Ã©quipe de taille modeste, Ã§a risque dâ€™Ãªtre une dÃ©cision peu pertinente Ã  moins dâ€™avoir la chance de disposer de quelques ingÃ©nieurs qui connaissent trÃ¨s bien Bazel et qui peuvent le gÃ©rer pour les autres.

## Nix

AprÃ¨s la publication des parties 1 et 2 de la sÃ©rie, plusieurs personnes sont venues me voir pour Ã©voquer avec enthousiasme le [gestionnaire de package Nix](https://nixos.org/nix/). Jâ€™ai donc dÃ©cidÃ© de me pencher dessus et de lui consacrer une section entiÃ¨re dans cette derniÃ¨re partie.

_Spoiler alert_ : oui, **Nix peut vraiment vous aider Ã  amÃ©liorer vos builds, mais la courbe dâ€™apprentissage est raide**. Peut-Ãªtre pas autant que pour Bazel, mais pas loin. Vous allez devoir apprendre Nix, ses concepts, son [langage dâ€™expression](https://nixos.wiki/wiki/Nix_Expression_Language) dÃ©diÃ©, puis bien sÃ»r comment lâ€™utiliser pour packager votre code dans votre langage et votre framework prÃ©fÃ©rÃ©s (jetez un Å“il au [manuel des nixpkgs](https://nixos.org/nixpkgs/manual/#chap-language-support) pour voir Ã  quoi Ã§a ressemble).

Cela Ã©tant dit, jâ€™ai quand mÃªme envie de vous parler de Nix en dÃ©tail, pour deux raisons : **ses concepts centraux sont trÃ¨s puissants** (et peuvent nous donner de bonnes idÃ©es sur le packaging de nos applications de faÃ§on plus gÃ©nÃ©rale), et il existe un **projet particulier appelÃ© [Nixery](https://nixery.dev/)** qui sera un ajout utile Ã  notre arsenal (quâ€™on dÃ©cide dâ€™utiliser Nix ou pas).

### Nix, de quoi sâ€™agit-il ?

Jâ€™ai entendu parler de Nix pour la premiÃ¨re fois il y a en gros 10 ans, alors que jâ€™assistais Ã  un talk aux RMLL â€œNixOS-The-Only-Functional-GNU-Linux-Distributionâ€. Ã€ lâ€™Ã©poque, câ€™Ã©tait dÃ©jÃ  trÃ¨s solide. Autrement dit, on nâ€™est pas du tout sur un truc de hipster avec rien derriÃ¨re.

CommenÃ§ons par un peu de terminologie :

- Nix est un _package manager_ Ã  installer sur nâ€™importe quelle machine Linux ou macOS ;
- NixOS est une _distribution Linux_ basÃ©e sur Nix ;
- `nixpkgs` est une collection de packages pour Nix ;
- Une â€œdÃ©rivationâ€ est une recette de build avec Nix.

Nix est un _package manager_ fonctionnel. **â€œFonctionnelâ€** signifie que chaque package est dÃ©fini par ses *inputsâ€ (code source, dÃ©pendancesâ€¦) et par sa _dÃ©rivation_ (recette de build). Rien dâ€™autre. Si on utilise les mÃªmes inputs avec la mÃªme dÃ©rivation, on aura toujours le mÃªme output. Ã€ lâ€™inverse, Ã  chaque fois quâ€™on modifie quelque chose sur un des inputs (mise Ã  jour du code source, modification des dÃ©pendances), ou alors sur la dÃ©rivation, lâ€™output est diffÃ©rent. Ca parait logique, non ? Et si Ã§a vous rappelle le cache de build Docker, câ€™est normalâ€¦ câ€™est exactement la mÃªme idÃ©e !

Sur un systÃ¨me traditionnel, quand un package dÃ©pend dâ€™un autre, la dÃ©pendance est gÃ©nÃ©ralement dÃ©crite de maniÃ¨re souple, non stricte. Par exemple, sur Debian, [python3.8](https://packages.debian.org/bullseye/python3.8) dÃ©pend de `python3.8-minimal (= 3.8.2-1)`, qui dÃ©pend lui mÃªme de `libc6 (>= 2.29)`. En parallÃ¨le, [ruby2.5](https://packages.debian.org/bullseye/ruby2.5) dÃ©pend de `libc6 (>= 2.17)`. Du coup, on installe une seule version de `libc6` et gÃ©nÃ©ralement Ã§a fonctionne.

Sur Nix, les package dÃ©pendent des **versions _exactes_ de ces bibliothÃ¨ques** et un mÃ©canisme trÃ¨s malin permet Ã  **chaque programme dâ€™utiliser son propre ensemble de bibliothÃ¨ques sans conflit avec les autres**. (Si vous vous demandez comment Ã§a marche : les programmes sâ€™appuient sur un _linker_ configurÃ© pour utiliser les bibliothÃ¨ques Ã  des chemins spÃ©cifiques. Conceptuellement, câ€™est comme lorsquâ€™on spÃ©cifie `#!/usr/local/bin/my-custom-python-3.8` pour lancer un script Python avec une version particuliÃ¨re de lâ€™interprÃ©teur.)

Par exemple, quand un programme utilise la bibliothÃ¨que C, sur un systÃ¨me classique le chemin serait `/usr/lib/libc.so.6`, alors quâ€™avec Nix, Ã§a pourrait Ãªtre `/nix/store/6yaj...drnn-glibc-2.27/lib/libc.so.6`.

Vous voyez ce chemin `/nix/store` ? Câ€™est le _Nix store_ (Ã©tonnant !). Nix y stocke des fichiers et rÃ©pertoires immuables, chacun identifiÃ© par son hash. Ã‡a se rapproche des layers utilisÃ©s par Docker, avec une grosse diffÃ©rence : les layers Docker sâ€™appliquent les uns par dessus les autres, alors que les fichiers et rÃ©pertoires de Nix sont totalement disjoints, il nâ€™y a jamais aucun conflit entre eux (puisque chaque objet est stockÃ© dans un rÃ©pertoire sÃ©parÃ©). Ce dÃ©tail aura son importance plus tard.

Sous Nix, lorsquâ€™on veut installer un package, on tÃ©lÃ©charge en fait un certain nombre de fichiers et de rÃ©pertoires quâ€™on dÃ©pose dans le _Nix store_ puis on configure un [profil](https://nixos.org/nix/manual/#sec-profiles) (en gros, câ€™est un paquet de liens symboliques pour que les programmes quâ€™on installe soient accessibles depuis notre variable `$PATH`).

### Passons Ã  lâ€™expÃ©rimentation de Nix !

Jusquâ€™Ã  prÃ©sent, on Ã©tait dans la thÃ©orie. Voyons maintenant **Nix en action** !

On peut lancer Nix dans un conteneur avec `docker run -ti nixos/nix`. Puis on peut lister les packages installÃ©s avec les commandes `nix-env --query` ou `nix-env -q`. Ã€ ce stade, on ne voit que `nix` et `nss-cacert`. Ã‡a peut paraÃ®tre bizarre : oÃ¹ sont passÃ©s le shell et les outils habituels comme `ls` ou autres ? Et bien dans cette image de conteneur, ils sont dans un exÃ©cutable statique, appelÃ© `busybox`.

Bon, on va peut-Ãªtre installer quelque chose ?

On exÃ©cute `nix-env --install redis` ou `niv-env -i redis`. La sortie de cette commande nous montre quâ€™on rÃ©cupÃ¨re de nouveaux â€œpathsâ€ et quâ€™ils sont stockÃ©s dans le store Nix. On va avoir au moins un â€œpathâ€ pour redis lui-mÃªme et trÃ¨s probablement un autre pour `glibc`. Nix utilise lui aussi glibc (pour le binaire `nix-env` et quelques autres), mais dans une version diffÃ©rente de celle utilisÃ©e par redis. Si on lance `ls -ld /nix/store/*glibc*/` on aura deux rÃ©pertoires qui correspondent aux deux versions de glibc. Pendant que jâ€™Ã©cris ces lignes, jâ€™ai deux versions de `glibc-2.27` :

```
ef5936ea667f:/# ls -ld /nix/store/*glibc*/
dr-xr-xr-x    ... /nix/store/681354n3k44r8z90m35hm8945vsp95h1-glibc-2.27/
dr-xr-xr-x    ... /nix/store/6yaj6n8l925xxfbcd65gzqx3dz7idrnn-glibc-2.27/
```

Vous allez me dire : â€œAttends, mais ce sont pas les mÃªmes versions ?â€. Et bien oui et non ! Câ€™est bien le mÃªme numÃ©ro, mais ces versions ont sÃ»rement Ã©tÃ© construites avec des options ou des patchs lÃ©gÃ¨rement diffÃ©rents. **Du point de vue de Nix, ce sont donc deux objets distincts**. Câ€™est exactement comme quand on build son programme avec le mÃªme Dockerfile alors quâ€™on a modifiÃ© une ligne de code quelque part : le builder Docker identifie ces petites diffÃ©rences et nous donne des images diffÃ©rentes.

On peut aussi **demander Ã  Nix de nous donner les dÃ©pendances de nâ€™importe quel fichier situÃ© dans le store Nix**. Pour Ã§a, on utilise les commandes `nix-store --query --references` ou `nix-store -qR`. Par exemple, pour voir les dÃ©pendances des binaires redis quâ€™on vient dâ€™installer juste avant, on peut faire un `nix-store -qR $(which redis-server)`.

Dans mon conteneur, lâ€™output ressemble Ã  Ã§a :

```
/nix/store/6yaj6n8l925xxfbcd65gzqx3dz7idrnn-glibc-2.27
/nix/store/mzqjf58zasr7237g8x9hcs44p6nvmdv7-redis-5.0.5
```

Et lÃ , Ã§a va Ãªtre magique. Si on veut lancer Redis, on a besoin seulement de ces rÃ©pertoires. Rien dâ€™autre. Ã‡a veut dire quâ€™on peut dÃ©poser ces rÃ©pertoires dans `scratch`, et Ã§a _marche_. On nâ€™a besoin dâ€™aucune librairie supplÃ©mentaire.

On peut gÃ©nÃ©raliser cette procÃ©dure en utilisant un _profil_ Nix. Un profil contient le rÃ©pertoire `bin` Ã  ajouter Ã  notre `$PATH` (et quelques autres trucs, mais je simplifie pour expliquer plus facilement). Si on fait `nix-env --profile myprof -i redis memcached`, le rÃ©pertoire `myprof/bin` va donc contenir les exÃ©cutables pour Redis et pour Memcached.

Encore mieux, les profils sont eux aussi stockÃ©s dans le store. Du coup on peut lancer la commande `nix-store -qR` pour lister les dÃ©pendances dâ€™un profil.

### CrÃ©er des images minimales avec Nix

Si on met bout Ã  bout tout ce que jâ€™ai expliquÃ© dans la section prÃ©cÃ©dente, on peut Ã©crire le Dockerfile suivant :

```
FROM nixos/nix
RUN mkdir -p /output/store
RUN nix-env --profile /output/profile -i redis
RUN cp -va $(nix-store -qR /output/profile) /output/store
FROM scratch
COPY --from=0 /output/store /nix/store
COPY --from=0 /output/profile/ /usr/local/
```

La premiÃ¨re phase utilise Nix pour installer Redis dans un nouveau _profil_. Ensuite on lui demande de lister toutes les dÃ©pendances de ce profil (la commande `nix-store -qR`) et on copie ces dÃ©pendances dans `/output/store`.

La seconde phase copie ces dÃ©pendances dans lâ€™image finale, dans `/nix/store` (c.a.d. leur place dâ€™origine dans Nix), ainsi que le profil lui mÃªme. (Copier le profil nous permet de rÃ©cupÃ©rer un rÃ©pertoire `bin` qui contient tous les binaires quâ€™on veut dans notre `$PATH`. Pratique.)

Le rÃ©sultat est une image de 35Mo avec Redis et _rien dâ€™autre_. Si on veut aussi un shell, on peut simplement mettre Ã  jour le Dockerfile avec un `-i redis bash`, et voilÃ  !

Si vous Ãªtes tentÃ© de rÃ©-Ã©crire tous vos Dockerfiles de cette faÃ§on lÃ , attendez un peu. DÃ©jÃ , cette image ne contient pas certaines metadata cruciales comme `VOLUME`, `EXPOSE`, ou encore `ENTRYPOINT` et son wrapper associÃ©. Et surtout jâ€™ai encore mieux pour vous dans la section suivante â€¦

### Nixery

Tous les gestionnaires de package fonctionnent de la mÃªme faÃ§on : ils tÃ©lÃ©chargent (ou gÃ©nÃ¨rent) des fichiers et les installent sur notre systÃ¨me. Mais avec Nix, il y a une diffÃ©rence importante : les fichiers installÃ©s sont immuable. Quand on installe des packages avec Nix, Ã§a nâ€™a aucun impact sur ce quâ€™on avait fait juste avant. Les couches Docker peuvent sâ€™affecter mutuellement (parce quâ€™une couche peut changer ou supprimer un fichier de la couche prÃ©cÃ©dente), mais ce nâ€™est pas le cas des objets stockÃ©s dans le Nix store.

Si on regarde le conteneur Nix quâ€™on a lancÃ© un peu plus tÃ´t (ou quâ€™on en lance un nouveau avec `docker run -ti nixos/nix`), dans le rÃ©pertoire `/nix/store`, on va trouver une liste de rÃ©pertoires comme ceux-lÃ  :

```
b7x2qjfs6k1xk4p74zzs9kyznv29zap6-bzip2-1.0.6.0.1-bin/
cinw572b38aln37glr0zb8lxwrgaffl4-bash-4.4-p23/
d9s1kq1bnwqgxwcvv4zrc36ysnxg8gv7-coreutils-8.30/
```

Si on utilisait Nix pour construire une image (comme on lâ€™a fait avec le Dockerfile plus haut), on aurait juste besoin de ces rÃ©pertoires dans `/nix/store` et dâ€™un paquet de liens symboliques.

Maintenant, imaginez quâ€™on stocke chaque rÃ©pertoire du Nix store comme un _layer_ dans une _registry_ Docker.

Ensuite, quand on a besoin dâ€™une image avec les packages X, Y et Z, il suffit de :

- gÃ©nÃ©rer un petit layer avec les liens symboliques Ã©voquÃ©s, permettant dâ€™invoquer facilement les programmes contenus dans ces packages X, Y et Z (ce qui correspond Ã  la derniÃ¨re ligne `COPY` dans le Dockerfile prÃ©cÃ©dent) ;
- demander Ã  Nix quels sont les objets du store correspondant Ã  X, Y et Z ainsi quâ€™Ã  leurs dÃ©pendances ;
- gÃ©nÃ©rer un manifest dâ€™image Docker qui rÃ©fÃ©rence tous ces layers.

Câ€™est exactement ce que [Nixery](https://nixery.dev/) fait. Nixery, câ€™est un registre de conteneurs â€œmagiqueâ€ : il **gÃ©nÃ¨re Ã  la volÃ©e des manifests dâ€™images de conteneurs qui rÃ©fÃ©rencent des layers qui correspondent aux objets du Nix store**.

ConcrÃ¨tement, si on fait un `docker run -ti nixery.dev/redis/memcached/bash bash`, on obtient un shell dans un conteneur avec Redis, Memcached et Bash. Ce qui est beau, câ€™est que lâ€™image de ce conteneur est gÃ©nÃ©rÃ©e Ã  la volÃ©e, _sans avoir besoin de la construire_. (Notez quâ€™on devrait plutÃ´t faire un `docker run -ti nixery.dev/shell/redis/memcached sh`, car quand une image commence par `shell`, Nixery nous donne quelques packages essentiels en plus du shell, par exemple `coreutils`).

Nixery implÃ©mente quelques optimisations en prime (notamment pour gÃ©rer la limite maximale de nombre de _layers_ dans Docker). Si vous Ãªtes intÃ©ressÃ© vous pouvez lire [ce blog post](https://grahamc.com/blog/nix-and-layered-docker-images) ou [ce talk de NixConf](https://www.youtube.com/watch?v=pOI9H4oeXqA).

### Dâ€™autres maniÃ¨res dâ€™utiliser Nix

On peut aussi utiliser Nix directement pour gÃ©nÃ©rer des images de conteneurs. Vous trouverez un bon exemple dans ce [blog post](https://lethalman.blogspot.com/2016/04/cheap-docker-images-with-nix_15.html).

Notez nÃ©anmoins que la technique Ã©voquÃ©e ici [nÃ©cessite kvm](https://twitter.com/jpetazzo/status/1241741547751845888), ce qui veut dire quâ€™elle ne fonctionnera pas dans des instances Cloud ou dans des conteneurs (sauf en utilisant de la virtualisation imbriquÃ©e, mais câ€™est encore trÃ¨s rare). Apparemment, il faut adapter les exemples et [utiliser buildLayeredImage](https://twitter.com/tazjin/status/1241743569888649218) mais nâ€™ayant pas investiguÃ© plus loin, je ne peux pas prÃ©ciser la quantitÃ© de travail et la complexitÃ©.

### To Nix or not to Nix ?

Ce blog post ayant vocation Ã  rester court (on me dit dans lâ€™oreillette que câ€™est ratÃ©ğŸ˜…), je nâ€™ai pas la place dâ€™expliquer en dÃ©tail comment utiliser Nix pour gÃ©nÃ©rer lâ€™image parfaite. Mais jâ€™espÃ¨re que cette petite dÃ©mo de Nix vous permet de vous faire une idÃ©e. Ã€ vous dâ€™approfondir le sujet si Ã§a vous a ouvert lâ€™appÃ©tit !

Pour ma part, je compte bien utiliser Nixery quand jâ€™ai besoin dâ€™images de conteneurs pour un usage prÃ©cis, en particulier avec Kubernetes. Prenons un exemple, si jâ€™ai besoin dâ€™une image avec `curl`, `tar` et la CLI de AWS. Mon approche traditionnelle aurait Ã©tÃ© dâ€™utiliser `alpine` puis dâ€™exÃ©cuter `pip install awscli`. Mais avec Nixery, je vais simplement pouvoir utiliser lâ€™image `nixery.dev/shell/curl/gnutar/awscli` !

## Avant de conclure, quelques dÃ©tails importantsâ€¦

Lorsquâ€™on utilise des images vraiment trÃ¨s rÃ©duites (comme `scratch`, ou dâ€™une certaine maniÃ¨re `alpine`, ou mÃªme certaines images gÃ©nÃ©rÃ©es avec distroless, Bazel ou Nix), on peut Ãªtre confrontÃ© Ã  des **erreurs inattendues**. Il y a des fichiers auxquels on ne pense pas en gÃ©nÃ©ral, mais qui sont supposÃ©s se trouver sur tout systÃ¨me UNIX bien Ã©levÃ©, y compris dans un conteneur.

De quels fichiers parle-t-on exactement ? Et bien, voici une petite liste non exhaustive :

- les certificats TLS ;
- les fichiers de timezone ;
- la base dâ€™utilisateurs et groupes (UID/GID).

Voyons de quoi il sâ€™agit plus prÃ©cisÃ©ment, de quand nous pouvons en avoir besoin, et comment on peut les ajouter Ã  nos images.

### Les certificats TLS

Quand on Ã©tablit une connection TLS vers un serveur distant (par exemple, une requÃªte vers un service web ou vers une API en HTTPS), en gÃ©nÃ©ral il nous prÃ©sente son certificat. En gÃ©nÃ©ral, ce certificat a Ã©tÃ© signÃ© par une _autoritÃ© de certification_ reconnue. En gÃ©nÃ©ral, aussi, on veut vÃ©rifier que ce certificat est valide et quâ€™on connait lâ€™autoritÃ© qui lâ€™a signÃ©.

(Je dis â€œen gÃ©nÃ©ralâ€ car il y a de rares scÃ©narios oÃ¹ on Ã§a nous est Ã©gal, ou bien oÃ¹ lâ€™on valide la connection diffÃ©remment. Mais si vous Ãªtes dans un de ces cas-lÃ , vous devez Ãªtre au courant. Sinon, par dÃ©faut, on part du principe quâ€™on _doit_ valider les certificats ! Et oui, la sÃ©curitÃ© avant tout !).

La clÃ© (sans jeu de mot) de la vÃ©rification des certificats rÃ©side dans ces fameuses autoritÃ©s de certification. Pour valider les certificats des serveurs auxquels on se connecte, on a besoin des certificats de ces autoritÃ©s. Ils sont typiquement installÃ©s dans `/etc/ssl`.

Si on utilise `scratch` ou une autre image minimale, et si on se connecte Ã  un serveur TLS, on peut obtenir des erreurs de validation des certificats. En Go, Ã§a ressemble Ã  ce message dâ€™erreur lÃ  `x509: certificate signed by unknown authority`. Si Ã§a nous arrive, il suffit de rajouter les certificats dans notre image. Pour Ã§a, on peut les rÃ©cupÃ©rer de quasiment toutes les images classiques comme `ubuntu` ou `alpine`. Peu importe laquelle, les systÃ¨mes ont quasiment tous le mÃªme groupe de certificats.

La ligne suivante va sâ€™en occuper :

```
COPY --from=alpine /etc/ssl /etc/ssl
```

On voit aussi que si on veut juste copier des fichiers depuis une image, on peut utiliser `--from` pour se rÃ©fÃ©rer Ã  cette image, sans rÃ©fÃ©rer Ã  un _build stage_ particulier !

### Les fichiers de timezone

Si notre code manipule la date et lâ€™heure, en particulier lâ€™heure _locale_ (par exemple, si on affiche lâ€™heure dans un fuseau horaire particulier, par opposition Ã  une horloge interne), on a besoin des _fichiers de timezone_. Vous pourriez vous dire : â€œAttends, mais comment Ã§a JÃ©rÃ´me ? Si je veux manipuler les fuseaux horaires, jâ€™ai seulement besoin dâ€™un offset par rapport Ã  UTC ou GMT !â€. Oui, sauf que â€¦ il y a les changements dâ€™heure dâ€™Ã©tÃ© et dâ€™heure dâ€™hiver. LÃ , les choses se compliquent trÃ¨s vite, car diffÃ©rents pays ou rÃ©gions nâ€™ont pas tous un passage Ã  lâ€™heure dâ€™Ã©tÃ©, ou bien pas au mÃªme moment. Par exemple, au sein mÃªme de lâ€™Utah aux Ã‰tats-Unis, les rÃ¨gles changent selon quâ€™on est en territoire Navajo ou pas ! Et puis par ailleurs, au fil des annÃ©es, ces rÃ¨gles Ã©voluent. En Europe par exemple, le changement dâ€™heure devrait Ãªtre supprimÃ© par tous les pays en 2021.

Revenons-en Ã  notre problÃ©matique technique. Si on veut pouvoir afficher lâ€™heure locale, on va avoir besoin des fichiers qui dÃ©crivent ces informations. Sur UNIX, ce sont les fichiers `tzinfo` ou `zoneinfo`, on les trouve gÃ©nÃ©ralement dans le rÃ©pertoire `/usr/share/zoneinfo`.

Certaines images (par exemple `centos` ou `debian`) incluent nativement ces fichiers de timezone. Dâ€™autres non, comme `alpine` ou `ubuntu`, et le package qui inclut ces fichiers sâ€™appelle gÃ©nÃ©ralement `tzdata`.

Pour installer les fichiers de timezone dans notre image, on peut par exemple lancer :

```
COPY --from=debian /usr/share/zoneinfo /usr/share/zoneinfo
```

Ou si on utilise dÃ©jÃ  `alpine`, on peut simplement faire un `apk add tzdata`.

Pour vÃ©rifier que les fichiers de timezone sont bien installÃ©s, on peut faire cette commande dans notre conteneur :

Si on obtient quelque chose comme `Fri Mar 13 21:03:17 CET 2020`, on est bon. Si on obtient `UTC`, Ã§a signifie que les fichiers de timezone nâ€™ont pas Ã©tÃ© trouvÃ©s.

### Les fichiers de mapping UID/GID

Une autre chose dont notre programme pourrait avoir besoin : rechercher les identifiants systÃ¨me des utilisateurs (User ID / UID) et de groupes dâ€™utilisateurs (Group ID / GID). GÃ©nÃ©ralement, Ã§a se passe dans `/etc/passwd` et `/etc/group`.

Le seul cas oÃ¹ jâ€™ai Ã©tÃ© obligÃ© de fournir ces fichiers explicitement, câ€™Ã©tait en exÃ©cutant des applications de bureau dans des conteneurs (en utilisant des outils comme [clink](https://github.com/soulshake/clink) ou les [dockerfiles](https://github.com/jessfraz/dockerfiles) de [Jessica Frazelle](https://twitter.com/jessfraz).

Si vous avez besoin dâ€™installer ces fichiers dans un conteneur, vous pouvez les gÃ©nÃ©rer localement, ou dans un conteneur (single-stage ou multistage), ou alors vous pouvez faire un _bind-mount_ depuis lâ€™hÃ´te (tout dÃ©pend de ce que vous Ãªtes en train de faire).

[Ce blog post](https://medium.com/@chemidy/create-the-smallest-and-secured-golang-docker-image-based-on-scratch-4752223b7324) nous montre comment ajouter un utilisateur Ã  un conteneur de build, et ensuite copier les fichiers `/etc/passwd` et `/etc/group` au conteneur de run.

## Conclusions

Comme on a pu le voir, il existe plein de mÃ©thodes pour rÃ©duire la taille de nos images. Vous vous demandez probablement laquelle est la meilleure de faÃ§on absolue ? Et bien jâ€™ai une mauvaise nouvelle : il nâ€™y en a pas. Comme toujours, la rÃ©ponse est _Ã§a dÃ©pend_.

Les **multi-stage builds basÃ©s sur Alpine** vont donner dâ€™excellents rÃ©sultats dans de nombreux scÃ©narios. Mais certaines bibliothÃ¨ques ne vont pas Ãªtre disponibles sur Alpine, et les compiler Ã  la main peut prendre du temps. Dans ce cas un multi-stage build avec une distribution classique va faire le job plus efficacement.

Les mÃ©canismes comme **Distroless** ou **Bazel** peuvent apporter des rÃ©sultats encore meilleurs, mais ils vont souvent nÃ©cessiter un investissement significatif en amont qui nâ€™est pertinent que pour certaines organisations.

Les **binaires statiques et lâ€™image `scratch`** peuvent Ãªtre bien utiles lorsquâ€™on dÃ©ploie dans des environnements de trÃ¨s petite taille, comme des systÃ¨mes embarquÃ©s par exemple.

Un dernier point important pour finir â€¦ Si on build et maintient de nombreuses images (des centaines, voire plus), on peut vouloir sâ€™en tenir Ã  une seule technique, mÃªme si elle nâ€™est pas optimale dans tous les cas. Câ€™est en effet souvent plus confortable de maintenir des **centaines dâ€™images avec une structure similaire** plutÃ´t que dâ€™avoir des plÃ©thores de variations avec des builds exotiques ou des Dockerfiles de niche.

On approche de la fin de ce tour dâ€™horizon sur lâ€™optimisation des images Docker. Avant de vous laisser, sachez que **si vous utilisez des techniques que je nâ€™ai pas mentionnÃ©es** dans cette sÃ©rie dâ€™article, je serai ravi de continuer Ã  en discuter avec vous, alors [contactez-moi](mailto:jerome.petazzoni@gmail.com?subject=Cherie+jai+retreci+docker+post) et rÃ©parons ces oublis !

### Mot de fin et remerciements

Lâ€™idÃ©e dâ€™Ã©crire cette sÃ©rie dâ€™articles mâ€™est venue en voyant ce [tweet](https://twitter.com/ellenkorbes/status/1216458929636630533) de [@ellenkorbes](https://twitter.com/ellenkorbes). Pendant mes formations sur les conteneurs, je passe toujours un peu de temps Ã  expliquer comment rÃ©duire la taille des images. Pour Ã©voquer le linking statique vs dynamique par exemple, je suis obligÃ© de prendre des raccourcis pour ne pas y passer la journÃ©e. Ca laisse un peu un goÃ»t dâ€™inachevÃ© et mâ€™a fait me demander sâ€™il Ã©tait finalement si nÃ©cessaire dâ€™Ã©voquer tous ces dÃ©tails. Puis quand jâ€™ai vu le **tweet dâ€™Ellen** et certaines de vos rÃ©ponses, je me suis dit : â€œWow, mais en fait Ã§a pourrait aider pas mal de monde si jâ€™Ã©crivais tout ce que jâ€™ai pu expÃ©rimenter ces derniÃ¨res annÃ©es sur ces sujets !â€.

â€¦ Je ne sais plus trop ce qui sâ€™est passÃ© ensuite, mais je me suis **rÃ©veillÃ© Ã  cÃ´tÃ© dâ€™une caisse de Club Mate vide et de trois blog posts** ! ğŸ¤·ğŸ»

Si vous cherchez des ressources gÃ©niales pour faire tourner du Go sur Kubernetes (et autres sujets connexes), je vous recommande vivement de consulter la [liste des talks](http://ellenkorbes.com/#talks) proposÃ©e par Ellen. La plupart de ces talks sont [disponibles sur Youtube](https://www.youtube.com/results?search_query=ellen+korbes), et je vous promets que câ€™est un trÃ¨s bon investissement de votre temps. En particulier, si vous avez aimÃ© cette sÃ©rie sur la taille des images Docker, vous pouvez regarder son futur talk [The Quest for the Fastest Deployment Time!](https://www.youtube.com/watch?v=WgliN_9j91g)

Un **grand merci aussi aux nombreuses personnes qui mâ€™ont fait des retours et des suggestions dâ€™amÃ©lioration** ! Je pense en particulier Ã  :

- [David DelabassÃ©e](https://twitter.com/delabassee) pour ses conseils sur Java et `jlink` ;
- [Sylvain Rabot](https://twitter.com/sylr) pour les certificats, les timezone et les fichiers UID et GID ;
- [Gleb Peregud](https://twitter.com/gleber) et [Vincent Ambo](https://twitter.com/tazjin) pour le partage de ressources trÃ¨s utiles Ã  propos de Nix.

Ces articles ont Ã©tÃ© Ã©crits originalement en anglais. La **version anglaise a Ã©tÃ© relue** par [AJ Bowen](https://twitter.com/s0ulshake). Elle a corrigÃ© de nombreuses fautes de frappe et pas mal dâ€™erreurs, contribuant Ã  amÃ©liorer significativement ce que jâ€™avais Ã©crit. Toutes les erreurs restantes sont les miennes. AJ travaille en ce moment sur un projet de prÃ©servation de cartes postales anciennes et historiques. Si Ã§a vous intÃ©resse, je vous invite vivement Ã  vous inscrire [ici](https://www.ephemerasearch.com/) pour en savoir plus.

La version franÃ§aise de cette sÃ©rie a Ã©tÃ© **traduite** par [AurÃ©lien Violet](https://twitter.com/brimstone75) et par [Romain Degez](https://twitter.com/rdegez). Si vous avez aimÃ© cette version franÃ§aise, vous pouvez leur tÃ©moigner un _grand merci_, Ã§a a reprÃ©sentÃ© bien plus de travail quâ€™il nâ€™y paraÃ®t !

---

La version anglaise de cet article est disponible [ici](https://www.ardanlabs.com/blog/2020/04/docker-images-part3-going-farther-reduce-image-size.html).

---

Ne ratez pas nos prochains articles DevOps et Cloud Native! Suivez Enix surÂ [Twitter](https://twitter.com/enixsas)!