---
link: https://medium.com/@tpierrain/monolithes-modulaires-activez-la-couche-r%C3%A9seau-d%C3%A8s-le-d%C3%A9but-45285eb84000
byline: Thomas Pierrain. (Ï…Ñ•e caÑ•e drÎ¹ven)
site: Medium
date: 2024-07-12T19:48
excerpt: TLDR; Pour garantir une sÃ©paration de modules efficace dans un
  monolithe modulaire, activez le transport rÃ©seau au runtime et en production
  dÃ¨s le dÃ©but, et ce mÃªme si tous les modules sont dÃ©ployÃ©sâ€¦
twitter: https://twitter.com/@tpierrain
slurped: 2024-08-26T18:52
title: "Monolithes modulaires : activez la couche rÃ©seau dÃ¨s le dÃ©but"
---
**_TLDR;_** _Pour garantir une sÃ©paration de modules efficace dans un monolithe modulaire, activez le transport rÃ©seau au runtime et en production dÃ¨s le dÃ©but, et ce mÃªme si tous les modules sont dÃ©ployÃ©s dans le mÃªme processus. Cela permet de dÃ©tecter et dâ€™amÃ©liorer les interactions inefficaces dÃ¨s le dÃ©but, Ã©vitant des problÃ¨mes de performance et de communication lors dâ€™un futur dÃ©ploiement sÃ©parÃ© des modules._

## Le problÃ¨me

Travailler dans un monolithe modulaire est passionnant et trÃ¨s utile, mais cela exige une grande rigueur. En effet, garantir la possibilitÃ© de sÃ©parer les modules Ã  lâ€™exÃ©cution est loin dâ€™Ãªtre simple. De nombreux piÃ¨ges peuvent survenir quand on travaille dans un tel environnement (comme lâ€™utilisation dâ€™une mÃªme base de donnÃ©es par plusieurs modules par exemple).

Aujourdâ€™hui, je souhaite me concentrer sur un autre piÃ¨ge courant : celui dâ€™avoir des interactions in-proc excessivement bavardes (chatty) entre les modules, ou pire, lâ€™adoption dâ€™un style RPC (Remote Procedure Call) entre modules.

Ces interactions inefficaces peuvent compromettre les performances et remettre mÃªme en question la sÃ©paration des modules que lâ€™on souhaiterait rÃ©aliser ultÃ©rieurement.

## Burn the â€œRPC witchâ€!

Depuis des dÃ©cennies, nous savons que la promesse du RPC ([**Remote Procedure Call**](https://en.wikipedia.org/wiki/Remote_procedure_call)) â€” donner lâ€™illusion quâ€™un code appelle une mÃ©thode sur un objet local alors quâ€™il ne fait que proxy pour un objet situÃ© dans un autre processus possiblement lointain â€” nous mÃ¨ne Ã  une impasse.

Cette â€œleaky abstractionâ€ nuit trÃ¨s souvent gravement aux performances de nos applications. Sans nous en rendre compte, on risque facilement de multiplier les appels rÃ©seau au sein de boucles for ou dâ€™adopter des communications excessivement bavardes entre modules, ce qui dÃ©grade encore plus les performances.

Bref. A la place du paradigme RPC, explicitons plutÃ´t tous les implicites, et rendons visible dans notre code le fait quâ€™il peut y avoir des I/O et du rÃ©seaux.

## Une prise en compte souvent tardive et frustrante

Dans le cas dâ€™un monolithe modulaire que jâ€™Ã©voquais juste au dessus, on peut se rendre compte un peu tard que nos interactions entre modules sont tellement peu adaptÃ©es Ã  un mode de communication rÃ©seau (http ou mÃªme gRPC) quâ€™il va nous falloir les repenser.

- Regrouper/Chunker nos aller-retour Ã  lâ€™aide de DTO (qui pour rappel servent Ã  rÃ©duire le nombre dâ€™A-R rÃ©seaux entre 2 systÃ¨mes)
- Changer les contrats dâ€™Ã©changes entre modules etc

Je dis Â« _un peu tard_ Â», car câ€™est pile au moment oÃ¹ on va se dire : Â« _Ã§a aurait du sens quâ€™on dÃ©ploie finalement ces 2 modules indÃ©pendamment_ Â» quâ€™on ne pourra pas le faire facilement, ni sans travail ou efforts supplÃ©mentaires.

## **Un pattern que je pensais bien connaitre**

De mon cÃ´tÃ©, cela faisait des annÃ©es que je travaille dans des monolithes modulaires en composant des Â« hexagones Â» dans le mÃªme processus pour cadrer les frontiÃ¨res et prÃ©parer ces Ã©ventuelles sÃ©parations.

Câ€™est un pattern que jâ€™ai appelÃ© the Hive (la ruche) et que nous sommes quelques-uns Ã  avoir expÃ©rimentÃ© depuis des annÃ©es (dont mon ami [**Julien TopÃ§u**](https://twitter.com/JulienTopcu), avec qui on va prÃ©senter celui-ci dans un talk trÃ¨s prochainement).

_the hive pattern, bientÃ´t sur vos Ã©crans ;-)_

Jusquâ€™alors, je pensais que la mise en Å“uvre de ce pattern Ã©tait suffisante pour cadrer ces frontiÃ¨res entre modules. On fait Ã©merger des ports (contrats) entre les modules qui marquent les frontiÃ¨res, et on peut au runtime utiliser :

- soit des Adapter In-Proc (c.a.d. Des adaptateurs qui ne font que des appels de mÃ©thodes dans le mÃªme processus, sur les ports dâ€™autres modules)
- soit des adaptateurs plus classiques, qui font des appels HTTP pour parler aux autres modules, lorsquâ€™on souhaite dÃ©ployer ces modules dans des processus diffÃ©rents.

## Sympa la forceâ€¦

Et bien je viens de me rendre compte tout rÃ©cemment que câ€™Ã©tait autre chose qui mâ€™avait protÃ©gÃ© jusquâ€™Ã  maintenant contre ces problÃ¨mes dâ€™interactions inadaptÃ©es au second mode de dÃ©ploiement (i.e.: quand on met du rÃ©seau et des I/O entre les modules).

Jâ€™avais jusquâ€™Ã  maintenant tout simplement Ã©tÃ©â€¦

## AidÃ© par mes prÃ©cÃ©dents environnements

Dans mon cas, câ€™est plutÃ´t que mon utilisation des monolithes modulaires a longtemps Ã©tÃ© cadrÃ© par des solutions de messaging low latency quâ€™on retrouve dans la finance (ex: 29West, renommÃ© Ultra Messaging). Ceux-ci permettaient Ã  la configuration de switcher facilement dâ€™un protocole Ã  lâ€™autre mais sans changer notre code, juste par de la configuration. On pouvait par exemple choisir une couche de transport parmis :

- IPC (via Shared-memory et ring buffer)
- UDP unicast
- UDP multicast (super pour de gros throughput combinÃ© Ã  de la faible latence)
- TCP (pour les infrastructures peu compatibles avec UDP)

La consÃ©quence câ€™est quâ€™on savait -lorsquâ€™on codait- que chaque Ã©change inter-module allait probablement passer par du rÃ©seau et tout ce que Ã§a implique. On avait donc fortement conscience de Ã§a et faisions explicitement du message-passing entre nos modules (et surtout pas du RPC).

En dâ€™autres termes, on rendait trÃ¨s explicite dans notre code le fait que lâ€™on parlait Ã  un module possiblement dÃ©ployÃ© ailleurs (et donc coÃ»teux en termes de latence, soumis Ã  des alÃ©as liÃ©s au rÃ©seau, etc).

## Mais Ã§a, câ€™Ã©tait avantâ€¦

Je constate aujourdâ€™hui que ce nâ€™est pas forcÃ©ment le cas pour des gens qui nâ€™ont connu le monolithe modulaire quâ€™Ã  travers des architectures Ã  base dâ€™API web ou qui utilisent mÃªme du gRPC (qui ne force malheureusement pas les gens Ã  faire du message passing explicite).

Du coup, ma proposition pour vous Ã©viter de devoir justifier des dÃ©lais supplÃ©mentaires et passer des semaines Ã  revoir et retailler certains de vos protocoles dâ€™Ã©changes entre modules, câ€™est :

> **Activez une couche de transport rÃ©seau au runtime (ex: HTTP), Y COMPRIS quand ce nâ€™est pas nÃ©cessaire en production et que vous dÃ©ployez tous vos modules dans le mÃªme processus/pod.**

Et par exemple pour nous qui dÃ©ployons tous nos services sur du K8s, cela signifie Ã©galement quâ€™on doit sâ€™assurer :

- De repasser par la partie Load Balancing (LB) sur tous nos appels HTTP internes entre modules dans le mÃªme monolithe (Ã§a rÃ©partit bien la charge, Ã©vite pleins dâ€™erreurs liÃ©s au graceful shutdown, mais aussi nous fait bÃ©nÃ©ficier des stratÃ©gies de throttling au niveau de lâ€™infra => nous Ã©vite de nous auto-DDoS avec des appels en boucle avec nous-mÃªme)
- De dÃ©sactiver le tcp-keep alive automatique sous-jacent Ã  nos implÃ©mentations de client HTTP pour tous les cas oÃ¹ on sâ€™appelle entre services ou modules dans le mÃªme cluster K8s. Ã‡a, câ€™est plutÃ´t liÃ© au mode de fonctionnement de Kube et nous permet justement de repasser systÃ©matiquement par le LB, mais surtout dâ€™Ã©viter de maintenir illusoirement des connexions vers des pods qui se sont dÃ©jÃ  fait prÃ©empter par exemple (Ã©lasticitÃ© et finops oblige). On sâ€™Ã©vite un bon paquet dâ€™erreur 503 et de retry. Attention par contre, Ã  ne pas dÃ©sactiver le tcp-keep alive si vous consommez un autre module dÃ©ployÃ© dans une autre rÃ©gion. Parce que dans ce cas prÃ©cis, vous allez sentir passer le coÃ»t du handshake TCP systÃ©matique ğŸ˜„

## Ok mais les performances dans tout Ã§a ?!?

Comme dâ€™hab, il vous suffit de mesurer plutÃ´t que dâ€™imaginer quelque chose (et cette heuristique lÃ  -elle â€” est toujours valable ğŸ™‚). En dâ€™autres termes :

Et mÃªme si vous dÃ©ployez ensuite tous vos modules sÃ©parÃ©ment, mais dans le mÃªme cluster K8s comme câ€™est le cas chez nous, lâ€™impact de faire des I/Os sera vraiment fortement limitÃ©, voire insensible pour des utilisateurs. En tout cas, le fait de vous Ãªtre entrainÃ©s Ã  passer par le rÃ©seau mÃªme quand ces modules Ã©taient regroupÃ©s dans le mÃªme processus, vous aura servi de rÃ©pÃ©tition pour ce dÃ©ploiement sÃ©parÃ© Ã  venir.

## Un feedback nÃ©cessaire

Et si ce nâ€™est pas le cas et que vous constatez des problÃ¨mes de latence sur certains usages, câ€™est que vous devriez trÃ¨s vraisemblablement revoir vos Ã©changes et contrats dâ€™interfaces (â€œportâ€ au sens archi hexa) entre modules.

Mais le fait dâ€™introduire du rÃ©seau dÃ¨s le dÃ©but entre vos modules comme si vous vouliez dÃ¨s maintenant les dÃ©ployer sÃ©parÃ©ment, aura une vertu cardinale : faire du back-pressure trÃ¨s tÃ´t contre vos mauvais design, vos mauvaises idÃ©es et vos mauvais contrats dâ€™interactions entre modules au sein de votre monolithe distribuÃ©.

Je ne sais pas pour vous, mais en ce qui me concerne : **avoir du feedback le plus vite possible sur des mauvais choix de design est quelque chose qui me rÃ©jouit ğŸ™‚.**